# ML  
### L16 Unsupervised Learning - Autoencoder  
- Deep Autoencoder  
中间的瓶颈层是需要的code，train的方法就是back propagation  
可以控制对称的参数相同，以减少一半的参数  
在MNIST上，先降维再恢复，Deep Autoencoder的结果要比PCA好  
可以用于文章检索，相似图片搜索，可应用于DNN初始参数的选择  
- Autoencoder for CNN  
Unpooling Deconvolution  
Deconvolution 只需要添加所需的0，然后用逆序的weight进行convolution  

### L17 Unsupervised Learning - Deep Generation Model  
- PixelRNN可以用于图像残缺部分的预测，或者影像后面序列的预测  
- 宝可梦创造  
把类似的颜色聚类，聚成167种，每张图用向量表示，用一个LSTM训练  
- VAE  
![a](http://or2urvelu.bkt.clouddn.com/L17-1.png)  

### L18 Unsupervised Learning - Deep Generation Model  
- VAE相比于Autoencoder，相当于在encoder时加了noise  
VAE相当于Gaussian Mixture Model的distributed representation版本  
- 最大化likelihood，红线项是KL divergence,是两个分布间的距离  
![a](http://or2urvelu.bkt.clouddn.com/L18-1.png)  
Find p(x|z) and q(z|x) maximizing Lb  
q(z|x) will be an approximation of p(z|x) in the end  
![a](http://or2urvelu.bkt.clouddn.com/L18-2.png)  
这一过程就相当于auto-encoder  
![a](http://or2urvelu.bkt.clouddn.com/L18-3.png)  
Conditional VAE可以画出风格相近的数字图片  
- GAN  用gradient descent来训练，要Fix the discriminator  
![a](http://or2urvelu.bkt.clouddn.com/L18-4.png)  

### L19 Transfer Learning  
1. Target data、Source data都是有labeled
    - Model Fine-tuning，Conservative Training 加正则项，防止过拟合  
    - Layer Transfer，把部分layer的参数转移  
    在图像处理上，复用前面几层效果比较好  
    Multitask Learning 用于多种语言识别  
2. Source data是labeled，Target data是unlabeled  
    - Domain-adversarial training，将MNIST用于MNIST-M上  
    ![a](http://or2urvelu.bkt.clouddn.com/L19-1.png)  
    - Zero-shot Learning  
    让不同语言相同语义的句子在同一区域，可以做多语言翻译  
    抽取attribute，然后进行Attribute embedding  
    ![a](http://or2urvelu.bkt.clouddn.com/L19-2.png)  

### L20 SVM  
- Hinge Loss + Kernel Method = SVM  
- 对比几种Loss Function，如下图  
cross entropy对比Square loss梯度较大，努力可以有回报  
Hinge Loss对比cross entropy，到一就变零，及格就好  
![a](http://or2urvelu.bkt.clouddn.com/L20-1.png)  
- Linear SVM，Loss fuction是凸函数，可以gradient descent  
![a](http://or2urvelu.bkt.clouddn.com/L20-2.png)  
gradient descent更新参数的方法  
![a](http://or2urvelu.bkt.clouddn.com/L20-3.png)  
Linear SVM – another formulation  
![a](http://or2urvelu.bkt.clouddn.com/L20-4.png)  
- Kernel Method  
Hinge loss很多为0，a很多为0，矩阵是稀疏的，不为0的就是support vectors  
![a](http://or2urvelu.bkt.clouddn.com/L20-5.png)  
Kernel Trick  
![a](http://or2urvelu.bkt.clouddn.com/L20-6.png)  
直接计算K会比较快，同理还有RBF Kernel和Sigmoid Kernel  
Kernel function是类似于相似度的  
![a](http://or2urvelu.bkt.clouddn.com/L20-7.png)  


### L21 Structured Learning  
- Unified Framework包含Object Detection、Summarization、Retrieval等实例  
- 三个问题  
    1. Evaluation，很难想象F(X,Y)长什么样  
    2. Inference，怎么解决arg max的问题  
    3. Training，在训练时，怎么找到F(X,Y)  
    ![a](http://or2urvelu.bkt.clouddn.com/L21-1.png)  


#### L22 Structured Learning - Linear Model  
- Structured Perceptron算法流程，例子见视频  
![a](http://or2urvelu.bkt.clouddn.com/L22-1.png)  

### L23  Structured Support Vector Machine
- separable case  
在separable情况下，获取w只需要有限次的更新，证明见视频  
separable指的是存在一个y能找到对应的w使其和y形成feature vector的内积最大  
- Non-separable Case  内积不是最大的  
虽然cost有max但只有交界不能微分，所以可以用Gradient Descent  
![a](http://or2urvelu.bkt.clouddn.com/L23-1.png)  
如果learning rate设1，就是structured perceptron的式子  
![a](http://or2urvelu.bkt.clouddn.com/L23-2.png)  
Another Cost Function，增加考虑错误的结果  
![a](http://or2urvelu.bkt.clouddn.com/L23-3.png)  
Another Viewpoint，![equation](http://latex.codecogs.com/gif.latex?$C^'$)没法直接求，找一个上界C  
![a](http://or2urvelu.bkt.clouddn.com/L23-4.png)  
- Regularization  
![a](http://or2urvelu.bkt.clouddn.com/L23-5.png)  
- Structured SVM  
通过推导和改造问题变成Structured SVM，蓝框是被改造的部分  
![equation](http://latex.codecogs.com/gif.latex?$\epsilon$) 为了放宽限制，叫Slack variable，大于零但是越小越好  
![a](http://or2urvelu.bkt.clouddn.com/L23-6.png)  
- Cutting Plane Algorithm  
用迭代的方法添加限制，解Quadratic Programming，再找most violated constraints  
![a](http://or2urvelu.bkt.clouddn.com/L23-7.png)  
- Binary SVM和Multi-class SVM 都可以用Structured SVM表示  
![a](http://or2urvelu.bkt.clouddn.com/L23-8.png)  

### Structured Learning - Sequence Labeling  
- HMM  
两个步骤  
![a](http://or2urvelu.bkt.clouddn.com/L24-1.png)  
概率可以用统计的方法计算，求y转换成求步骤二的式子最大，可应用Viterbi算法  
![a](http://or2urvelu.bkt.clouddn.com/L24-2.png)  
HMM - Drawbacks  
会脑补他没看过的东西，可能预测出现训练数据没有的数据  
CRF可以解决这个问题  
- CRF  
不能取等号，流程见视频  
![a](http://or2urvelu.bkt.clouddn.com/L24-3.png)  
Feature Vector有两部分组成  
求解几率relations between tags and words和relations between tags  
CRF – Training Criterion  
最大化正确结果，最小化其他结果  
![a](http://or2urvelu.bkt.clouddn.com/L24-4.png)  
如果s,t在正确中出现次数多，就增加w  
如果在任意的x,y中出现的多，就减小w  
所有的y一撇可以用viterbi算法  
![a](http://or2urvelu.bkt.clouddn.com/L24-5.png)  
CRF Training  
![a](http://or2urvelu.bkt.clouddn.com/L24-6.png)  
CRF – Inference  
![a](http://or2urvelu.bkt.clouddn.com/L24-7.png)  
CRF - Summary  
![a](http://or2urvelu.bkt.clouddn.com/L24-8.png)  
- Structured Perceptron   
第一步可以用相同的feature vector  
![a](http://or2urvelu.bkt.clouddn.com/L24-9.png)  
Structured Perceptron减去的不是所有的y，而是使feature vector最大的  
![a](http://or2urvelu.bkt.clouddn.com/L24-10.png)  
- Structured SVM  
要考虑margin和error，解法有两种  
![a](http://or2urvelu.bkt.clouddn.com/L24-11.png)  
需解决下面问题，定义![equation](http://latex.codecogs.com/gif.latex?$\Delta$)为错误率，就可以用Viterbi解  
![a](http://or2urvelu.bkt.clouddn.com/L24-12.png)  
- 对比RNN LSTM  
RNN LSTM不能考虑整个序列，但有多向RNN  
HMM CRF可以考虑label依赖性  
RNN的cost和error不总相关  
RNN是deep的，表现更好  
- Concluding Remarks  
![a](http://or2urvelu.bkt.clouddn.com/L24-13.png)  


































