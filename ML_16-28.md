# ML  
### L16 Unsupervised Learning - Autoencoder  
- Deep Autoencoder  
中间的瓶颈层是需要的code，train的方法就是back propagation  
可以控制对称的参数相同，以减少一半的参数  
在MNIST上，先降维再恢复，Deep Autoencoder的结果要比PCA好  
可以用于文章检索，相似图片搜索，可应用于DNN初始参数的选择  
- Autoencoder for CNN  
Unpooling Deconvolution  
Deconvolution 只需要添加所需的0，然后用逆序的weight进行convolution  

### L17 Unsupervised Learning - Deep Generation Model  
- PixelRNN可以用于图像残缺部分的预测，或者影像后面序列的预测  
- 宝可梦创造  
把类似的颜色聚类，聚成167种，每张图用向量表示，用一个LSTM训练  
- VAE  
![a](http://or2urvelu.bkt.clouddn.com/L17-1.png)  

### L18 Unsupervised Learning - Deep Generation Model  
- VAE相比于Autoencoder，相当于在encoder时加了noise  
VAE相当于Gaussian Mixture Model的distributed representation版本  
- 最大化likelihood，红线项是KL divergence,是两个分布间的距离  
![a](http://or2urvelu.bkt.clouddn.com/L18-1.png)  
Find p(x|z) and q(z|x) maximizing Lb  
q(z|x) will be an approximation of p(z|x) in the end  
![a](http://or2urvelu.bkt.clouddn.com/L18-2.png)  
这一过程就相当于auto-encoder  
![a](http://or2urvelu.bkt.clouddn.com/L18-3.png)  
Conditional VAE可以画出风格相近的数字图片  
- GAN  用gradient descent来训练，要Fix the discriminator  
![a](http://or2urvelu.bkt.clouddn.com/L18-4.png)  

### L19 Transfer Learning  
1. Target data、Source data都是有labeled
    - Model Fine-tuning，Conservative Training 加正则项，防止过拟合  
    - Layer Transfer，把部分layer的参数转移  
    在图像处理上，复用前面几层效果比较好  
    Multitask Learning 用于多种语言识别  
2. Source data是labeled，Target data是unlabeled  
    - Domain-adversarial training  将MNIST用于MNIST-M上  
    ![a](http://or2urvelu.bkt.clouddn.com/L19-1.png)  
    - Zero-shot Learning  
    让不同语言相同语义的句子在同一区域，可以做多语言翻译  
    抽取attribute，然后进行Attribute embedding  
    ![a](http://or2urvelu.bkt.clouddn.com/L19-2.png)  

### L20 SVM  
Hinge Loss+Kernel Method = SVM














